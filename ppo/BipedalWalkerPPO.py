#Basic PPO implementation for BipedalWalker-v3 environment with video recording generated by Gemini 3
# Run: ./.venv/bin/python ppo/BipedalWalkerPPO.py
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.monitor import Monitor
import os

class PointTrackerCallback(BaseCallback):
    def __init__(self, check_freq_steps=400, verbose=1):
        super(PointTrackerCallback, self).__init__(verbose)
        self.check_freq_steps = check_freq_steps
        self.current_episode_reward = 0
        self.episode_steps = 0

    def _on_step(self) -> bool:
        reward = self.locals['rewards'][0]
        self.current_episode_reward += reward
        self.episode_steps += 1
        if self.episode_steps % self.check_freq_steps == 0:
            print(f"-> Step {self.episode_steps}: Current Points = {self.current_episode_reward:.2f}")
        if self.locals['dones'][0]:
            print(f"--- Training Episode Finished | Final Score: {self.current_episode_reward:.2f} ---")
            self.current_episode_reward = 0
            self.episode_steps = 0
        return True

video_folder = "logs/basic_PPO/"
os.makedirs(video_folder, exist_ok=True)

def make_train_env():
    return Monitor(gym.make("BipedalWalker-v3"))

train_env = DummyVecEnv([make_train_env])

model = PPO(
    "MlpPolicy", 
    train_env, 
    verbose=1,
    learning_rate=0.0003,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
)

print("Starting training...")
model.learn(total_timesteps=500000, callback=PointTrackerCallback())
model.save("ppo_bipedal_walker_model")
train_env.close()

#Testing
print("\n" + "="*50)
print("TESTING PHASE")
print("="*50)

def make_test_env():
    return Monitor(gym.make("BipedalWalker-v3", render_mode="rgb_array"))

test_env = DummyVecEnv([make_test_env])

test_env = VecVideoRecorder(
    test_env, 
    video_folder,
    record_video_trigger=lambda x: x == 0, 
    video_length=2000,
    name_prefix="ppo-bipedal-walker-eval"
)

num_test_episodes = 5
test_scores = []

for i in range(num_test_episodes):
    obs = test_env.reset()
    done = False
    episode_reward = 0
    
    while not done:
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, info = test_env.step(action)
        episode_reward += reward[0]
        
        if done:
            score = info[0]['episode']['r']
            test_scores.append(score)
            print(f"Test Episode {i+1}: Official Score = {score:.2f}")

# Report
avg_score = sum(test_scores) / len(test_scores)
print("\n" + "-"*30)
print(f"AVERAGE TEST SCORE: {avg_score:.2f}")
print(f"Videos saved to: {video_folder}")
print("-" * 30 + "\n")

test_env.close()

#Etrophy Loss: measuring randomness. if it gets lower, that is good since it means the agent is getting less confused
#Explained_variance: Tells if the agent understands the relationship between its actions and the rewards. Closer to 1 means it gets it.
#Std: agent narrowing down leg swing range
#explained_variance: How well the agent can predict the reward it's about to get.
#value_loss: How wrong the agent was about its reward prediction.